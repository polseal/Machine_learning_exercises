# -*- coding: utf-8 -*-
"""Sheet2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1nn2RU1cMt1ffmfwKQ0MGZycWyrUNgkRx
"""

from sklearn import datasets
from sklearn.manifold import TSNE
import pandas as pd
import numpy as np
np.random.seed(0)
from seaborn import scatterplot, color_palette
from sklearn.model_selection import train_test_split,GridSearchCV
from sklearn.preprocessing import  MinMaxScaler, StandardScaler
from sklearn import svm, tree
from sklearn.metrics import classification_report, confusion_matrix
from sklearn.tree import DecisionTreeClassifier
import matplotlib.pyplot as plt
from sklearn.utils._testing import ignore_warnings
from sklearn.exceptions import ConvergenceWarning, FitFailedWarning
from sklearn.preprocessing import PolynomialFeatures
import warnings
from sklearn.tree import DecisionTreeRegressor
from sklearn.datasets import make_classification
import warnings
warnings.filterwarnings("ignore")

print('1', '_'*50)

forge_data = datasets.fetch_openml('banknote-authentication')

df = pd.DataFrame(forge_data.data, columns = forge_data.feature_names)
target = forge_data.target
print(target.value_counts())

tsne = TSNE(n_components=2, verbose=1, random_state=0)

df_embedded = tsne.fit_transform(df)

df['feature_1'] = df_embedded[:,0]
df['feature_2'] = df_embedded[:,1]

scatterplot(x="feature_1", y="feature_2", hue=target.tolist(),
                palette=color_palette("hls", 2),
                data=df).set(title="Banknote authenticatiobn T-SNE projection")

print('We can split the set with a line, but would have to accept some % or errors')

df = pd.DataFrame(forge_data.data, columns = forge_data.feature_names)
X_train, X_test, y_train, y_test = train_test_split(df.values, target, test_size=0.2, random_state=0)

minMaxScaler = MinMaxScaler()

X_train = minMaxScaler.fit_transform(X_train)
X_test = minMaxScaler.fit_transform(X_test)

clf = svm.LinearSVC(C=1,max_iter=10000, loss='hinge', random_state =0)

clf = clf.fit(X_train, y_train)

y_pred = clf.predict(X_test)

conf_matrix = pd.DataFrame(confusion_matrix(y_test,y_pred))
print(conf_matrix)
class_report = classification_report(y_test, y_pred, labels=clf.classes_)
print(class_report)

y_pred_1 = clf.predict(X_train)
conf_matrix = pd.DataFrame(confusion_matrix(y_train, y_pred_1))
print(conf_matrix)
class_report = classification_report(y_train, y_pred_1, labels=clf.classes_)
print(class_report)

print("The model doesn't over- or underfit")

print('2', '_'*50)

bc_data = datasets.load_breast_cancer()
df = pd.DataFrame(bc_data.data, columns = bc_data.feature_names)
target = bc_data.target
print(np.unique(target, return_counts=True))
df.shape

X_train, X_test, y_train, y_test = train_test_split(df.values, target, stratify=target, test_size=0.2, random_state=0)

minMaxScaler = MinMaxScaler()
X_train = minMaxScaler.fit_transform(X_train)
X_test = minMaxScaler.fit_transform(X_test)

print('Pipeline 1')

clf = svm.LinearSVC(C=1,max_iter=10000, loss='hinge', random_state=0)
clf = clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
conf_matrix = pd.DataFrame(confusion_matrix(y_test,y_pred))
print(conf_matrix)
class_report = classification_report(y_test, y_pred, labels=clf.classes_)
print(class_report)

print('Pipeline 2')

clf = svm.LinearSVC(C=1,max_iter=10000, random_state=0)
transform = PolynomialFeatures(degree=3)
X_train_poly = transform.fit_transform(X_train)
X_test_poly = transform.fit_transform(X_test)
clf = clf.fit(X_train_poly, y_train)
y_pred = clf.predict(X_test_poly)
conf_matrix = pd.DataFrame(confusion_matrix(y_test,y_pred))
print(conf_matrix)
class_report = classification_report(y_test, y_pred, labels=clf.classes_)
print(class_report)

print('Pipeline 3')

clf = svm.SVC(C=1,kernel='poly', random_state=0, coef0=1)
clf = clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
conf_matrix = pd.DataFrame(confusion_matrix(y_test,y_pred))
print(conf_matrix)
class_report = classification_report(y_test, y_pred, labels=clf.classes_)
print(class_report)

#Grid search

grid_values = {'C':[0.1, 5, 0.35, 6.4, 7, 50, 171, 21, 13, 90] ,'max_iter':[1000,1200,5000,4500,9800,7780, 5430,1100,3000,9070, 10000], 'loss':['hinge','squared_hinge']}
svc_lin = svm.LinearSVC(random_state=0)
clf = GridSearchCV(svc_lin, grid_values)

clf = clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
conf_matrix = pd.DataFrame(confusion_matrix(y_test,y_pred))
print(conf_matrix)
class_report = classification_report(y_test, y_pred, labels=clf.classes_)
print(class_report)
print("best parameters are: ", clf.best_params_)

print('3', '_'*50)

X, y = make_classification(n_samples=10, n_features=5, n_classes=2, random_state=0)

def distance_n(X, n):
  rows_number = np.shape(X)[0]
  max = np.max(X[:,n])
  min = np.min(X[:,n])
  df = pd.DataFrame(index=range(rows_number),columns=range(rows_number))
  for v in range(0,rows_number):
    value = X[v,n]
    l = []
    for j in range(0,rows_number):
      l.append(np.round(np.abs(value - X[j,n])/(max-min),5))
      df.iloc[j,v] = np.round(np.abs(value - X[j,n])/(max-min),5)
  return df

def findNN (threshold, df, row):
  l = df[row].tolist()
  l_highest = sorted([(x,i) for (i,x) in enumerate(l) if x>threshold], reverse=True)[1:4]
  results = [x[1]+1 for x in l_highest]
  return results

distance0 = distance_n(X, 0)
exp_sim = distance0[:]
exp_sim = exp_sim.applymap(lambda x: np.exp(x * (-0.1)) )


NN1 = findNN(np.mean(exp_sim.mean()), exp_sim, 2)

distance1 = distance_n(X, 1)
exp_sim = distance1[:]
exp_sim = exp_sim.applymap(lambda x: np.exp(x * (-0.1)) )


NN2 = findNN(np.mean(exp_sim.mean()), exp_sim, 2)

distance2 = distance_n(X, 2)
exp_sim = distance2[:]
exp_sim = exp_sim.applymap(lambda x: np.exp(x * (-0.1)) )


NN3 = findNN(np.mean(exp_sim.mean()), exp_sim, 2)

distance3 = distance_n(X, 3)
exp_sim = distance3[:]
exp_sim = exp_sim.applymap(lambda x:1 if x<0.3 else 0)


NN4 = findNN(np.mean(exp_sim.mean()), exp_sim, 2)

distance4 = distance_n(X, 4)
exp_sim = distance4[:]
exp_sim = exp_sim.applymap(lambda x:1 if x<0.3 else 0)


NN5 = findNN(np.mean(exp_sim.mean()), exp_sim, 2)

l = NN1+NN2+NN3+NN4+NN5
print(f"The rows most similar to the 3rd one are {set([i for i in l if l.count(i)>1])}")

print('4', '_'*50)

forge_data = datasets.fetch_openml('banknote-authentication')

df = pd.DataFrame(forge_data.data, columns = forge_data.feature_names)
target = forge_data.target
df.columns.values

X_train, X_test, y_train, y_test = train_test_split(df.values, target, test_size=0.2, random_state=0)

clf = DecisionTreeClassifier(max_depth=2, random_state=0)

clf.fit(X_train, y_train)

colors = np.array(["orange", "green"])
t = np.where(y_train == '2', 0, 1)
plt.scatter(X_train[:, 0], X_train[:, 1], c = colors[t]);

plt.scatter(X_train[:, 0], X_train[:, 2], c = colors[t]);

fig = plt.figure(figsize=(7,7))
_ = tree.plot_tree(clf,
                   filled=True,
                   feature_names=forge_data.feature_names,
                   class_names=target.name)

print("There were 605 objects of C and 492 objects of I in the beginning, gini inpurity = 0.495 \nThen the set was split into two by V1 <=0.274.")
print("As a result, two nodes emerged. There are more C objects on the right, and I objects on the left, respectively.\nGini inpurity decreased.",
   "The node containing more C values is of a blue shade, the one with the majority of I values is of an orange shade.\nThe tree is built until the max depth is reached.")

clf = clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
conf_matrix = pd.DataFrame(confusion_matrix(y_test,y_pred))
print(conf_matrix)
class_report = classification_report(y_test, y_pred, labels=clf.classes_)
print(class_report)

#Grid search

grid_values = {'criterion':['gini','entropy'],'max_depth':[2,2.5,3,5.34,11,7.11,12,13.7,4.8,14,14.91], 'splitter':['best','random']}

dtc = DecisionTreeClassifier(random_state=0)
clf = GridSearchCV(dtc, grid_values)
clf = clf.fit(X_train, y_train)
y_pred = clf.predict(X_test)
conf_matrix = pd.DataFrame(confusion_matrix(y_test,y_pred))
print(conf_matrix)
class_report = classification_report(y_test, y_pred, labels=clf.classes_)
print(class_report)
print("best parameters are: ", clf.best_params_)

def createNoise(arr):
  l = np.shape(arr)
  mu, sigma = 0, 0.2
  s = np.random.normal(mu, sigma, l)
  return s

s1 = createNoise(y_train)
s2 = createNoise(y_test)
y_train_noise = np.add(s1, pd.to_numeric(y_train,errors='coerce').to_numpy())
y_test_noise = np.add(s2, pd.to_numeric(y_test,errors='coerce').to_numpy())


regressor = DecisionTreeRegressor(random_state=0)
regressor.fit(X_train, y_train_noise)

print(f"Mean accuracy {regressor.score(X_test, y_test_noise)}","\n")


r = DecisionTreeRegressor(random_state=0)
grid_values = {'criterion': ['squared_error', 'friedman_mse', 'absolute_error', 'poisson'], 'max_depth':[None, 2, 4, 5], 'min_samples_leaf':[1, 2.3, 3]}
clf = GridSearchCV(r, grid_values)
clf = clf.fit(X_train, y_train_noise)

print("best parameters are: ", clf.best_params_)
print(f"Mean accuracy {clf.score(X_test, y_test_noise)}")
