# -*- coding: utf-8 -*-
"""Copy of E3 final.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1RJjSVWtFdwBaTq7I1AkXrxhkk4Gpftii
"""

from sklearn import datasets
from sklearn.datasets import make_regression
import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import adjusted_rand_score, accuracy_score
from sklearn.ensemble import BaggingClassifier
import matplotlib.pyplot as plt
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import AdaBoostClassifier
from sklearn.linear_model import LinearRegression
from numpy.linalg import svd
from matplotlib import cm
from sklearn.decomposition import PCA

np.random.seed(0)

print('**TASK 1: BAGGING K-MEANS**')

bc_data = datasets.load_breast_cancer()
df = pd.DataFrame(bc_data.data, columns=bc_data.feature_names)
target = bc_data.target

X_train, X_test, y_train, y_test = train_test_split(df.values, target, test_size=0.2, random_state=0, stratify=target)

neigh = KNeighborsClassifier(n_neighbors=2)
neigh.fit(X_train, y_train)
y_pred = neigh.predict(X_test)

print(f"Adjusted Rand Index is a similarity measure. ARI = {adjusted_rand_score(y_test, y_pred)}.")

bagging = BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=2),
                            n_estimators=20,
                            max_samples=round(X_train.shape[0] * 0.3),
                            random_state=0)

clf = bagging.fit(X_train, y_train)
y_pred = clf.predict(X_test)
print(f"ARI = {adjusted_rand_score(y_test, y_pred)}.")

print('**TASK 2: RANDOM FOREST**')

X, Y = make_regression(n_samples=800, n_features=20, n_targets=3, n_informative=10, noise=0.2, random_state=0)

X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=0.2, random_state=0)

pipe1 = Pipeline(steps=[('scaler', StandardScaler()),
                        ('classifier', RandomForestRegressor(n_estimators=50, max_leaf_nodes=10, random_state=0))])
pipe1.fit(X_train, y_train)
print("Accuracy StandardScaler & RandomForestRegressor", pipe1.score(X_test, y_test))

pipe2 = Pipeline(steps=[('scaler', MinMaxScaler()),
                        ('classifier', RandomForestRegressor(n_estimators=50, max_leaf_nodes=10, random_state=0))])
pipe2.fit(X_train, y_train)
print("Accuracy MinMaxScaler & RandomForestRegressor", pipe2.score(X_test, y_test))

print(pipe1.steps[1][1].feature_importances_)

print(pipe2.steps[1][1].feature_importances_)

print('*TASK 3: ADABOOST WITH LOGISTIC REGRESSION**')

bc_data = datasets.load_breast_cancer()
df = pd.DataFrame(bc_data.data, columns=bc_data.feature_names)
target = bc_data.target

scaler = MinMaxScaler()
scaled_features = scaler.fit_transform(df.values)
# scaled_features_df = pd.DataFrame(scaled_features, index=df.index, columns=df.columns)

X_train, X_test, y_train, y_test = train_test_split(scaled_features, target, test_size=0.2, random_state=0,
                                                    stratify=target)

model = LogisticRegression(random_state=0)
model.fit(X_train, y_train)

print("Accuracy ", model.score(X_test, y_test))

model = AdaBoostClassifier(base_estimator=LogisticRegression(random_state=0), n_estimators=20)
model.fit(X_train, y_train)
print("Accuracy ", model.score(X_test, y_test))

print('**TASK 4: PRINCIPAL COMPONENT ANALYSIS FOR DIMENSIONALITY REDUCTION**')

X, Y = make_regression(n_samples=10, n_features=2, n_targets=1, random_state=0)

X_mc = X.copy()
X_mc -= np.mean(X_mc, axis=0)

X_train, X_test, y_train, y_test = train_test_split(X_mc, Y, test_size=0.2, random_state=0)

x = X_mc[:, 0]
y = X_mc[:, 1]
z = Y
fig = plt.figure(figsize=(7, 7))
ax = plt.axes(projection='3d')
ax.set_title('Scatter plot')
ax.scatter(x, y, z)
plt.show()

model = LinearRegression()
model.fit(X_train, y_train)


x_surf = X_mc[:, 0][:]
y_surf = X_mc[:, 1][:]
z_surf = model.coef_[0] * X_mc[:, 0] + model.coef_[1] * X_mc[:, 1] + model.intercept_
fig = plt.figure(figsize=(7, 7))
ax = plt.axes(projection='3d')
ax.set_title('Linear regression surface plot')
ax.scatter(x, y, z)
ax.plot_trisurf(x_surf, y_surf, z_surf, color=(0, 1, 0, 1))

plt.show()

U, S, Vt = svd(X_mc, full_matrices=True)

components = np.matmul(X_mc, np.transpose(Vt))

fig = plt.figure(figsize=(6, 6))
ax = plt.axes(projection='3d')
ax.set_title('PCA via SVD')
ax.scatter(components[:, 0], components[:, 1], c=Y)
plt.show()


pca = PCA()
components = pca.fit_transform(X_mc)
ax = plt.axes(projection='3d')
ax.set_title('PCA via sklearn decomposition')
ax.scatter(components[:, 0], components[:, 1], c=Y)
plt.show()
print("For some reason, the matrices are different (-1)*X")